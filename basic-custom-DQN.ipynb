{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q gym accelerate torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T03:04:47.303930Z","iopub.execute_input":"2024-11-14T03:04:47.304921Z","iopub.status.idle":"2024-11-14T03:04:59.491076Z","shell.execute_reply.started":"2024-11-14T03:04:47.304864Z","shell.execute_reply":"2024-11-14T03:04:59.489270Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import os\nimport numpy as np\nimport time\n\nimport matplotlib.pyplot as plt\n\n# Initialize env\nimport random\nfrom gym import Env, spaces \nfrom IPython.display import clear_output\n\n# Model\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\n\nrandom.seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T06:32:28.594281Z","iopub.execute_input":"2024-11-14T06:32:28.594843Z","iopub.status.idle":"2024-11-14T06:32:32.384326Z","shell.execute_reply.started":"2024-11-14T06:32:28.594792Z","shell.execute_reply":"2024-11-14T06:32:32.383203Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Game\n\nThere is a board 6x6. Objection to move the fastest way from Agent 1 to Goal 3 -> win, but if it catches Trap 2 -> lose.\n\n![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*zx8131dzXB2ku7DJr_QJOw.png)\n\n\n- The agent is the number 1\n- The green square is the number 2\n- The red square is the number 3\n- The empty squares are the number 0\n\nCause `gym` requires the environment state to be represented as a single row of values, not a grid of values, we must flatten the grid by taking the top row, adding the 2nd row to the end of it, adding the 3rd row to the end of those combined rows, etc. etc.\n\n![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*ydFqgzVgLGiBQj66JHUFkQ.png)\n\n\nThe `action` of `agent` must be converted to number too. I assume:\n\n- Up: 0\n- Down: 1\n- Right: 2\n- Left: 3\n\nWe also have to define `reward` of `agent` after finishing an `action`:\n\n- If the agent wins the game, it will be rewarded with 1 point.\n- If the agent loses the game, it will be rewarded with -1 point.\n- Every action that does not result in a win or a loss will give a reward of -0.01 points. This is to incentivize the agent to take the fewest actions to win the game\n\n**Notice:**\n\n`Gym` environments have 4 functions that need to be defined within the environment class:\n\n- __init__(self)\n- step(self, action)\n- reset(self)\n- render(self)","metadata":{}},{"cell_type":"code","source":"### ENVIRONMENT ###\nclass CustomEnv(Env):\n    def __init__(self):\n        # Initialize cumulative reward of agent\n        self.cumulative_reward = 0\n\n        self.env_dict = {\n            \"NOTHING\": 0,\n            \"PLAYER\": 1,\n            \"WIN\": 3,\n            \"LOSE\": 2\n        }\n        self.action_dict = {\n            \"UP\": 0,\n            \"DOWN\": 1,\n            \"LEFT\": 2,\n            \"RIGHT\": 3\n        }\n        \n        # Initialize board 6x6 in flatten shape or state\n        self.state = [self.env_dict['NOTHING']] * 36 \n\n        # Initialize random position of player, win and lose\n        position = np.random.choice(36, size = 3, replace=False)\n        self.player_position = position[0]\n        self.win_position = position[1]\n        self.lose_position = position[2]\n\n        # Assign position of player, win and lose to state\n        self.state[self.player_position] = self.env_dict['PLAYER']\n        self.state[self.win_position] = self.env_dict['WIN']\n        self.state[self.lose_position] = self.env_dict['LOSE']\n\n        # Convert to numpy array to afford requirement from gym library\n        self.state = np.array(self.state, dtype=np.int16)\n\n        # Initialize observation space of state\n        self.observation_space = spaces.Box(0, 3, [36,], dtype=np.int16)\n\n        # Define action agent can do (0-4)\n        self.action_space = spaces.Discrete(4) \n        \n    def step(self, action):\n        # placeholder for debugging information\n        info = {}\n\n        # Define origin value for parameters\n        done = False\n        reward = -0.01\n        previous_position = self.player_position\n\n        # Take action\n        if action == self.action_dict['UP']:\n            if (self.player_position - 6) >= 0:\n                self.player_position -= 6\n        elif action == self.action_dict['DOWN']:\n            if (self.player_position + 6) < 36:\n                self.player_position += 6\n        elif action == self.action_dict['LEFT']:\n            if (self.player_position % 6) != 0:\n                self.player_position -= 1\n        elif action == self.action_dict['RIGHT']:\n            if (self.player_position % 6) != 5:\n                self.player_position += 1\n        else:\n            # check for invalid actions\n            raise Exception(\"invalid action\")\n\n        # Check for win/lose statement\n        if self.state[self.player_position] == self.env_dict['WIN']:\n            reward = 1.0\n            self.cumulative_reward += reward\n            done = True\n            print(\"---------------------------------\")\n            print(f'Cumulative Reward: {self.cumulative_reward}')\n            print('YOU WIN!!!!')\n            print(\"---------------------------------\")\n\n        elif self.state[self.player_position] == self.env_dict['LOSE']:\n            reward = -10.0\n            self.cumulative_reward += reward\n            done = True\n            print(\"---------------------------------\")\n            print(f'Cumulative Reward: {self.cumulative_reward}')\n            print('YOU LOSE...')\n            print(\"---------------------------------\")\n\n        # Update the environment state\n        if not done:\n            self.state[previous_position] = self.env_dict['NOTHING']\n            self.state[self.player_position] = self.env_dict['PLAYER']\n            self.cumulative_reward += reward\n\n        return self.state, reward, done, info\n        \n    def reset(self):\n        \n        self.cumulative_reward = 0\n        \n        self.state = [self.env_dict['NOTHING']] * 36\n        \n        position = np.random.choice(36, size = 3, replace=False)\n        self.player_position = position[0]\n        self.win_position = position[1]\n        self.lose_position = position[2]\n\n        self.state[self.player_position] = self.env_dict['PLAYER']\n        self.state[self.win_position] = self.env_dict['WIN']\n        self.state[self.lose_position] = self.env_dict['LOSE']\n\n        self.state = np.array(self.state, dtype=np.int16)\n\n        return self.state\n\n        \n    def render(self, process: bool = False):\n        if process:\n            self._screen_print(self.state, self.cumulative_reward)\n        else:\n            self._clear_screen()\n            self._screen_print(self.state, self.cumulative_reward)\n\n    def _clear_screen(self):\n        clear_output()\n        os.system('cls')\n        \n    def _screen_print(self, state_array, cumulative_reward):\n        print(f'Cumulative Reward: {cumulative_reward}')\n        print()\n        for i in range(6):\n            for j in range(6):\n                print('{:4}'.format(state_array[i*6 + j]), end = \"\")\n            print()\n    \n    def observe(self):\n        return self.state\n\n    def show_action(self, action):\n        if action == self.action_dict['UP']:\n            print(\"UP\")\n        elif action == self.action_dict['DOWN']:\n            print(\"DOWN\")\n        elif action == self.action_dict['LEFT']:\n            print(\"LEFT\")\n        elif action == self.action_dict['RIGHT']:\n            print(\"RIGHT\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:02:43.516186Z","iopub.execute_input":"2024-11-14T09:02:43.516715Z","iopub.status.idle":"2024-11-14T09:02:43.551621Z","shell.execute_reply.started":"2024-11-14T09:02:43.516670Z","shell.execute_reply":"2024-11-14T09:02:43.549796Z"}},"outputs":[],"execution_count":169},{"cell_type":"code","source":"### Test env \nenv_object = CustomEnv()\nenv_object.render()\n\naction = int(input(\"Enter action:\"))\n\nstate, reward, done, info = env_object.step(action)\n\nwhile not done:\n    env_object.render()\n    action = int(input(\"Enter action:\"))\n    state, reward, done, info = env_object.step(action)\n\nenv_object.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T07:18:25.194102Z","iopub.execute_input":"2024-11-14T07:18:25.194528Z","iopub.status.idle":"2024-11-14T07:18:37.781715Z","shell.execute_reply.started":"2024-11-14T07:18:25.194490Z","shell.execute_reply":"2024-11-14T07:18:37.780406Z"}},"outputs":[{"name":"stderr","text":"sh: 1: cls: not found\n","output_type":"stream"},{"name":"stdout","text":"Cumulative Reward: -0.04\n\n   0   0   0   1   0   0\n   0   0   2   3   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter action: 1\n"},{"name":"stdout","text":"---------------------------------\nCumulative Reward: 0.96\nYOU WIN!!!!\n---------------------------------\n","output_type":"stream"}],"execution_count":71},{"cell_type":"markdown","source":"This is done for creating env. \n\nNow we will continue to train a RL Agent.\n\nThere are 2 approaches to solve a RL problem:\n\n- Policy-based: Try to build a policy that agent follow to solve the problem.\n- Value-based: Not using policy but only value from value function\n- Hybrid (actor-critic): combine two approaches above.\n\nIn this notebook, I will use value-approach using Q-value.\n\nEnvironment can be divided into 2 approaches:\n\n- Model-based: RL agent can model the environment, so it can predict what will happen if a new action happens: reward model, transition model. \n- Model-free: RL agent study to optimize action based on reward it gains after finishing one action. RL agent does not predict next action but choose a set of actions to minimum cumulative reward.\n\nThe agent can be `off-policy` or `on-policy`:\n\n- on-policy: Agent use policy to generate actions and learn from these actions but does not change the policy. Usually used in situations where policy has less changes.\n- off-policy: Agent can learn from actions that come from other policy.\n\nI will use `temporal difference learning` where the Q-value estimates will be learned by experience in replay buffer.","metadata":{}},{"cell_type":"code","source":"class DQN(nn.Module):\n    def __init__(self, input_dim=36, output_dim=4, nf = 64):\n        super(DQN, self).__init__()\n        \n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.nf = nf\n        \n        self.fc1 = nn.Linear(self.input_dim, self.nf)\n        self.fc2 = nn.Linear(self.nf, self.nf*2)\n        # self.fc3 = nn.Linear(self.nf*2, self.nf*4)\n        self.fc4 = nn.Linear(self.nf*2, self.output_dim)\n\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout(x)\n        # x = torch.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:14:54.491434Z","iopub.execute_input":"2024-11-14T09:14:54.492495Z","iopub.status.idle":"2024-11-14T09:14:54.503933Z","shell.execute_reply.started":"2024-11-14T09:14:54.492441Z","shell.execute_reply":"2024-11-14T09:14:54.502335Z"}},"outputs":[],"execution_count":186},{"cell_type":"code","source":"### Experience replay\nclass ExperienceReplay:\n    def __init__(self, max_memory: int = 10000):\n        self.max_memory = max_memory\n        self.memory = deque(maxlen=self.max_memory)\n\n    def add_experience(self, sars: list):\n        \"\"\"\n        Args:\n        sars (List): [previous_state, action, reward, current_state, done]\n        \"\"\"\n        self.memory.append(sars)\n        \n    def get_batch(self, env, batch_size = 8):\n        memory_length = len(self.memory)\n        env_dim = env.observation_space.shape[0]\n        true_batch_size = min(batch_size, memory_length)\n\n        idxes = np.random.choice(memory_length, size=true_batch_size, replace=False)\n        \n        batch_samples = list(zip(*[self.memory[idx] for idx in idxes]))\n        previous_states, actions, rewards, current_states, dones = (np.stack(sample) for sample in batch_samples)\n\n        return previous_states, current_states, actions, rewards, dones\n\n    def _getlen(self):\n        return len(self.memory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:02:52.539162Z","iopub.execute_input":"2024-11-14T09:02:52.539648Z","iopub.status.idle":"2024-11-14T09:02:52.550698Z","shell.execute_reply.started":"2024-11-14T09:02:52.539586Z","shell.execute_reply":"2024-11-14T09:02:52.549426Z"}},"outputs":[],"execution_count":171},{"cell_type":"code","source":"class DQNAgent:\n    def __init__(self, env, model, exp_replay, gamma, batch_size = 8, learning_rate = 0.001, epochs=10, epsilon=1.0):\n        # Initialize variables\n        self.model = model\n        self.env = env\n        self.exp_replay = exp_replay\n        self.gamma = gamma\n\n        # Train parameters\n        self.optimizer = optim.Adam(self.model.parameters(), lr = learning_rate)\n        self.criterion = nn.MSELoss()\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.epsilon = epsilon\n        self.loop = 0\n\n    def _getloss(self, x_previous: np.array, x_current: np.array, actions, rewards, dones):\n        \n        q_values = self.model(torch.tensor(x_previous, dtype=torch.float32))\n        q_targets = q_values.clone()\n        \n        # Bellman equation\n        current_q_values = self.model(torch.tensor(x_current, dtype=torch.float32))\n    \n        for i in range(x_previous.shape[0]):\n            q_targets[i, actions[i]] = rewards[i] + self.gamma * torch.max(current_q_values[i]) * ~dones[i]\n        \n        loss = self.criterion(q_values, q_targets)\n    \n        return loss\n     \n    def train(self):\n        print(\"Initializing model training\")\n        \n        for epoch in range(self.epochs):\n            self.env.reset()\n            current_state = self.env.observe()\n            \n            done = False\n            loop_count = 0\n            time_start = time.time()\n            epsilon = self.epsilon\n            \n            while not done:\n                previous_state = current_state\n                if random.random() < epsilon:\n                    action = self.env.action_space.sample()\n                else:\n                    with torch.no_grad():\n                        q = self.model(torch.tensor(previous_state, dtype=torch.float32))\n                        action = torch.argmax(q).item()\n    \n                current_state, reward, done, _ = self.env.step(action)\n    \n                exp_replay.add_experience(\n                    [previous_state, int(action), reward, current_state, done]\n                )\n    \n                previous_batch, target_batch, action_batch, \\\n                reward_batch, done_batch = exp_replay.get_batch(\n                    self.env, batch_size=self.batch_size\n                )\n\n                loss = self._getloss(\n                    previous_batch, target_batch, \n                    action_batch, reward_batch, done_batch\n                )\n                \n                loop_count += 1\n                \n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                # Gradient clipping \n                max_norm = 1.0 \n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm)\n                \n                epsilon = epsilon/(1 + 0.1*epsilon)\n                    \n            time_end = time.time()\n            print(f\"End of epochs: {epoch+1}, number of loop: {loop_count}, time taken: {round(time_end - time_start, 3)} seconds\")\n\n    def save_model(self, path=\"./saved_model.pth\"):\n        torch.save(model, path)\n        print(f\"Model saved to {path}\")\n\n    def load_model(self, path=None, train=False):\n        assert path is not None, \"Add model's path.\"\n        model = torch.load(path)\n        if train:\n            return model\n        else:\n            model.eval()\n            return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:15:01.627784Z","iopub.execute_input":"2024-11-14T09:15:01.628276Z","iopub.status.idle":"2024-11-14T09:15:01.654809Z","shell.execute_reply.started":"2024-11-14T09:15:01.628230Z","shell.execute_reply":"2024-11-14T09:15:01.652938Z"}},"outputs":[],"execution_count":187},{"cell_type":"code","source":"env_object = CustomEnv()\nmodel = DQN(env_object.observation_space.shape[0],4)\nexp_replay = ExperienceReplay()\n\nagent = DQNAgent(\n    env=env_object, \n    model=model, \n    exp_replay=exp_replay,\n    gamma=0.9,\n    batch_size = 64,\n    learning_rate = 0.001,\n    epochs = 20,\n    epsilon = 1.0\n)\n\n# Train\nagent.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:15:05.852811Z","iopub.execute_input":"2024-11-14T09:15:05.853265Z","iopub.status.idle":"2024-11-14T09:15:24.618968Z","shell.execute_reply.started":"2024-11-14T09:15:05.853224Z","shell.execute_reply":"2024-11-14T09:15:24.617878Z"}},"outputs":[{"name":"stdout","text":"Initializing model training\n---------------------------------\nCumulative Reward: 0.96\nYOU WIN!!!!\n---------------------------------\nEnd of epochs: 1, number of loop: 5, time taken: 0.036 seconds\n---------------------------------\nCumulative Reward: 0.99\nYOU WIN!!!!\n---------------------------------\nEnd of epochs: 2, number of loop: 2, time taken: 0.009 seconds\n---------------------------------\nCumulative Reward: -10.0\nYOU LOSE...\n---------------------------------\nEnd of epochs: 3, number of loop: 1, time taken: 0.006 seconds\n---------------------------------\nCumulative Reward: -10.1\nYOU LOSE...\n---------------------------------\nEnd of epochs: 4, number of loop: 11, time taken: 0.074 seconds\n---------------------------------\nCumulative Reward: -10.73\nYOU LOSE...\n---------------------------------\nEnd of epochs: 5, number of loop: 74, time taken: 1.126 seconds\n---------------------------------\nCumulative Reward: 0.37999999999999967\nYOU WIN!!!!\n---------------------------------\nEnd of epochs: 6, number of loop: 63, time taken: 1.129 seconds\n---------------------------------\nCumulative Reward: -10.23\nYOU LOSE...\n---------------------------------\nEnd of epochs: 7, number of loop: 24, time taken: 0.427 seconds\n---------------------------------\nCumulative Reward: -10.92\nYOU LOSE...\n---------------------------------\nEnd of epochs: 8, number of loop: 93, time taken: 1.76 seconds\n---------------------------------\nCumulative Reward: 0.47999999999999976\nYOU WIN!!!!\n---------------------------------\nEnd of epochs: 9, number of loop: 53, time taken: 0.988 seconds\n---------------------------------\nCumulative Reward: -0.020000000000000684\nYOU WIN!!!!\n---------------------------------\nEnd of epochs: 10, number of loop: 103, time taken: 1.913 seconds\n---------------------------------\nCumulative Reward: -10.46\nYOU LOSE...\n---------------------------------\nEnd of epochs: 11, number of loop: 47, time taken: 0.876 seconds\n---------------------------------\nCumulative Reward: 0.71\nYOU WIN!!!!\n---------------------------------\nEnd of epochs: 12, number of loop: 30, time taken: 0.542 seconds\n---------------------------------\nCumulative Reward: -10.1\nYOU LOSE...\n---------------------------------\nEnd of epochs: 13, number of loop: 11, time taken: 0.207 seconds\n---------------------------------\nCumulative Reward: -0.030000000000000693\nYOU WIN!!!!\n---------------------------------\nEnd of epochs: 14, number of loop: 104, time taken: 1.941 seconds\n---------------------------------\nCumulative Reward: -10.0\nYOU LOSE...\n---------------------------------\nEnd of epochs: 15, number of loop: 1, time taken: 0.018 seconds\n---------------------------------\nCumulative Reward: 0.44999999999999973\nYOU WIN!!!!\n---------------------------------\nEnd of epochs: 16, number of loop: 56, time taken: 1.01 seconds\n---------------------------------\nCumulative Reward: -10.07\nYOU LOSE...\n---------------------------------\nEnd of epochs: 17, number of loop: 8, time taken: 0.141 seconds\n---------------------------------\nCumulative Reward: 0.5099999999999998\nYOU WIN!!!!\n---------------------------------\nEnd of epochs: 18, number of loop: 50, time taken: 0.893 seconds\n---------------------------------\nCumulative Reward: -10.26\nYOU LOSE...\n---------------------------------\nEnd of epochs: 19, number of loop: 27, time taken: 0.54 seconds\n---------------------------------\nCumulative Reward: -12.819999999999984\nYOU LOSE...\n---------------------------------\nEnd of epochs: 20, number of loop: 283, time taken: 5.116 seconds\n","output_type":"stream"}],"execution_count":188},{"cell_type":"code","source":"exp_replay._getlen()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:16:59.550571Z","iopub.execute_input":"2024-11-14T09:16:59.551057Z","iopub.status.idle":"2024-11-14T09:16:59.559241Z","shell.execute_reply.started":"2024-11-14T09:16:59.551017Z","shell.execute_reply":"2024-11-14T09:16:59.557620Z"}},"outputs":[{"execution_count":190,"output_type":"execute_result","data":{"text/plain":"1046"},"metadata":{}}],"execution_count":190},{"cell_type":"code","source":"env_object2 = CustomEnv()\nprint(\"----Origin-----\")\nenv_object2.render(process=True)\nprevious_state = env_object2.state\n\nwith torch.no_grad():\n    action = torch.argmax(model(torch.tensor(previous_state, dtype=torch.float32))).item()\n\ncurrent_state, reward, done, info = env_object2.step(action)\n\nfor i in range(20):\n    print(\"-----------------------\")\n    env_object2.show_action(action)\n    env_object2.render(process=True)\n    with torch.no_grad():\n        action = torch.argmax(model(torch.tensor(current_state, dtype=torch.float32))).item()\n    current_state, reward, done, info = env_object2.step(action)\n    if done:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T09:15:28.265250Z","iopub.execute_input":"2024-11-14T09:15:28.265734Z","iopub.status.idle":"2024-11-14T09:15:28.285373Z","shell.execute_reply.started":"2024-11-14T09:15:28.265689Z","shell.execute_reply":"2024-11-14T09:15:28.283966Z"}},"outputs":[{"name":"stdout","text":"----Origin-----\nCumulative Reward: 0\n\n   0   0   3   0   2   0\n   0   0   0   0   0   0\n   0   0   1   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n-----------------------\nRIGHT\nCumulative Reward: -0.01\n\n   0   0   3   0   2   0\n   0   0   0   0   0   0\n   0   0   0   1   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n-----------------------\nLEFT\nCumulative Reward: -0.02\n\n   0   0   3   0   2   0\n   0   0   0   0   0   0\n   0   0   1   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n-----------------------\nDOWN\nCumulative Reward: -0.03\n\n   0   0   3   0   2   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n   0   0   1   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n-----------------------\nUP\nCumulative Reward: -0.04\n\n   0   0   3   0   2   0\n   0   0   0   0   0   0\n   0   0   1   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n-----------------------\nUP\nCumulative Reward: -0.05\n\n   0   0   3   0   2   0\n   0   0   1   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n   0   0   0   0   0   0\n---------------------------------\nCumulative Reward: 0.95\nYOU WIN!!!!\n---------------------------------\n","output_type":"stream"}],"execution_count":189}]}